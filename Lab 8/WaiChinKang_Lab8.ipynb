{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ba99c9-377a-4d35-bb5f-03cf007d1be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            0\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           0\n",
      "I love listening to music and going to concerts                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " playing\n",
      " the\n",
      " weekends\n",
      " on\n",
      " football\n",
      " video\n",
      " sports\n",
      " prefer\n",
      " over\n",
      " games\n",
      "\n",
      "Cluster 1:\n",
      " to\n",
      " and\n",
      " read\n",
      " watch\n",
      " movies\n",
      " like\n",
      " books\n",
      " concerts\n",
      " going\n",
      " music\n",
      "\n",
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF VECTORIZER\n",
    "\n",
    "#import the libraries\n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from tabulate import tabulate \n",
    "from collections import Counter\n",
    "\n",
    "#create the document\n",
    "dataset = [\"I love playing football on the weekends\", \n",
    "           \"I enjoy hiking and camping in the mountains\", \n",
    "           \"I like to read books and watch movies\", \n",
    "           \"I prefer playing video games over sports\", \n",
    "           \"I love listening to music and going to concerts\"]\n",
    "\n",
    "#vectorize the dataset\n",
    "vectorizer = TfidfVectorizer() \n",
    "X = vectorizer.fit_transform(dataset)\n",
    "\n",
    "#perform clustering\n",
    "k = 2  # Define the number of clusters \n",
    "km = KMeans(n_clusters=k) \n",
    "km.fit(X) \n",
    " \n",
    "# Predict the clusters for each document \n",
    "y_pred = km.predict(X) \n",
    " \n",
    "# Display the document and its predicted cluster in a table \n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]] \n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)]) \n",
    "print(tabulate(table_data, headers=\"firstrow\")) \n",
    "\n",
    "# Print top terms per cluster \n",
    "print(\"\\nTop terms per cluster:\") \n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "terms = vectorizer.get_feature_names_out() \n",
    "for i in range(k): \n",
    "    print(\"Cluster %d:\" % i) \n",
    "    for ind in order_centroids[i, :10]: \n",
    "        print(' %s' % terms[ind]) \n",
    "    print()\n",
    "\n",
    "# Calculate purity \n",
    "total_samples = len(y_pred) \n",
    "cluster_label_counts = [Counter(y_pred)] \n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples \n",
    "print(\"Purity:\", purity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0118ba61-6e1a-47d3-abf5-3818791eb154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           1\n",
      "I love listening to music and going to concerts                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " camping\n",
      " enjoy\n",
      " hiking\n",
      " mountains\n",
      " weekends\n",
      " listening\n",
      " concerts\n",
      " football\n",
      " games\n",
      " going\n",
      "\n",
      "Cluster 1:\n",
      " love\n",
      " playing\n",
      " football\n",
      " weekends\n",
      " going\n",
      " sports\n",
      " music\n",
      " concerts\n",
      " video\n",
      " games\n",
      "\n",
      "Purity: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Emily\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF VECTORIZER (after preprocessing)\n",
    "\n",
    "# import the libraries\n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from tabulate import tabulate \n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# text preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # remove punctuation and numbers\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# create the document and apply preprocessing\n",
    "raw_dataset = [\n",
    "    \"I love playing football on the weekends\", \n",
    "    \"I enjoy hiking and camping in the mountains\", \n",
    "    \"I like to read books and watch movies\", \n",
    "    \"I prefer playing video games over sports\", \n",
    "    \"I love listening to music and going to concerts\"\n",
    "]\n",
    "dataset = [preprocess(doc) for doc in raw_dataset]\n",
    "\n",
    "# vectorize the dataset\n",
    "vectorizer = TfidfVectorizer() \n",
    "X = vectorizer.fit_transform(dataset)\n",
    "\n",
    "# perform clustering\n",
    "k = 2  # Define the number of clusters \n",
    "km = KMeans(n_clusters=k, random_state=42) \n",
    "km.fit(X) \n",
    "\n",
    "# Predict the clusters for each document \n",
    "y_pred = km.predict(X) \n",
    "\n",
    "# Display the document and its predicted cluster in a table \n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]] \n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(raw_dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\")) \n",
    "\n",
    "# Print top terms per cluster \n",
    "print(\"\\nTop terms per cluster:\") \n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "terms = vectorizer.get_feature_names_out() \n",
    "for i in range(k): \n",
    "    print(\"Cluster %d:\" % i) \n",
    "    for ind in order_centroids[i, :10]: \n",
    "        print(' %s' % terms[ind]) \n",
    "    print()\n",
    "\n",
    "# Calculate purity \n",
    "total_samples = len(y_pred) \n",
    "cluster_label_counts = [Counter(y_pred)] \n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples \n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e03727-86fd-4466-b621-40bf6a2974de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\emily\\anaconda3\\lib\\site-packages (1.15.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\emily\\anaconda3\\lib\\site-packages (from scipy) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e0f78ba-544b-478f-a560-d23be9f6d103",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\Emily\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans \n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtabulate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tabulate \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (C:\\Users\\Emily\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans \n",
    "from gensim.models import Word2Vec \n",
    "from tabulate import tabulate \n",
    "from collections import Counter \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# original documents\n",
    "raw_dataset = [\n",
    "    \"I love playing football on the weekends\", \n",
    "    \"I enjoy hiking and camping in the mountains\", \n",
    "    \"I like to read books and watch movies\", \n",
    "    \"I prefer playing video games over sports\", \n",
    "    \"I love listening to music and going to concerts\"\n",
    "]\n",
    "\n",
    "# apply preprocessing\n",
    "dataset = [preprocess(doc) for doc in raw_dataset]\n",
    "\n",
    "# train Word2Vec model \n",
    "tokenized_dataset = [doc.split() for doc in dataset] \n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# create document embeddings \n",
    "X = np.array([\n",
    "    np.mean([word2vec_model.wv[word] for word in doc if word in word2vec_model.wv] or [np.zeros(100)], axis=0)\n",
    "    for doc in tokenized_dataset\n",
    "])\n",
    "\n",
    "# perform clustering\n",
    "k = 2  # number of clusters \n",
    "km = KMeans(n_clusters=k, random_state=42) \n",
    "km.fit(X) \n",
    "\n",
    "# predict clusters\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# tabulate original (non-preprocessed) document with its predicted cluster\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]] \n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(raw_dataset, y_pred)]) \n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# calculate purity\n",
    "total_samples = len(y_pred) \n",
    "cluster_label_counts = [Counter(y_pred)] \n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples \n",
    "print(\"Purity:\", purity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1c53574-9044-4a24-a474-c06c5064735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Emily\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " service\n",
      " since\n",
      " adding\n",
      " boxes\n",
      " second\n",
      " customer\n",
      " rude\n",
      " joke\n",
      " never\n",
      " malfunction\n",
      "\n",
      "Cluster 1:\n",
      " internet\n",
      " comcast\n",
      " rep\n",
      " tech\n",
      " security\n",
      " cable\n",
      " rude\n",
      " burial\n",
      " us\n",
      " mins\n",
      "\n",
      "Cluster 2:\n",
      " internet\n",
      " im\n",
      " dealing\n",
      " interruptions\n",
      " lead\n",
      " stopping\n",
      " late\n",
      " verizon\n",
      " multiple\n",
      " poor\n",
      "\n",
      "Cluster 3:\n",
      " mbps\n",
      " speed\n",
      " contract\n",
      " service\n",
      " day\n",
      " internet\n",
      " customer\n",
      " years\n",
      " blast\n",
      " call\n",
      "\n",
      "Cluster 4:\n",
      " contract\n",
      " would\n",
      " xfinity\n",
      " months\n",
      " time\n",
      " get\n",
      " terrible\n",
      " hardware\n",
      " signed\n",
      " customer\n",
      "\n",
      "\n",
      "Silhouette Score: 0.0004\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv(\"customer_complaints_1.csv\")\n",
    "df = df[['text']].dropna()  # Use lowercase column name\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Processed_Text'] = df['text'].apply(preprocess)\n",
    "\n",
    "# Limit for performance (optional)\n",
    "df = df.head(1000)\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Processed_Text'])\n",
    "\n",
    "# Perform KMeans clustering\n",
    "k = 5\n",
    "km = KMeans(n_clusters=k, random_state=42)\n",
    "km.fit(X)\n",
    "y_pred = km.predict(X)\n",
    "df['Cluster'] = y_pred\n",
    "\n",
    "# Print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\" {terms[ind]}\")\n",
    "    print()\n",
    "\n",
    "# Silhouette score as a proxy for clustering quality\n",
    "silhouette = silhouette_score(X, y_pred)\n",
    "print(f\"\\nSilhouette Score: {silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33b592-f8f2-4f62-8300-77185c96ffa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
