{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7ccaaa-8a94-48f7-ac64-fef92a3b0cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Emily\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Emily\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Emily\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Emily\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Emily\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\Emily\\AppData\\Local\\Temp\\ipykernel_7340\\1690541475.py:99: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  return BeautifulSoup(text, \"html.parser\").get_text()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0  Im happy with uniten actually, even the people...   \n",
      "1  I’m having a pretty good time here, happy to m...   \n",
      "2        a very neutral place in terms of everything   \n",
      "3  I would say Uniten it's  a good university  bu...   \n",
      "4   UNITEN is well-regarded, particularly for its...   \n",
      "\n",
      "                                           processed  \n",
      "0             im happy uniten actually even people w  \n",
      "1         i ’ m pretty good time happy meet w people  \n",
      "2                      neutral place term everything  \n",
      "3  would say uniten good university issue need im...  \n",
      "4  uniten wellregarded particularly strong engine...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re \n",
    "import emoji \n",
    "import string \n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords, wordnet \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag \n",
    "\n",
    "# Download required NLTK resources \n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')                    # For lemmatization \n",
    "nltk.download('omw-1.4')                     # WordNet lexical database \n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging \n",
    "nltk.download('punkt')                       # For tokenization \n",
    "\n",
    "# Initialize tools \n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# Dictionary of slang words and their replacements \n",
    "slang_dict = { \n",
    "    \"tbh\": \"to be honest\", \n",
    "    \"omg\": \"oh my god\", \n",
    "    \"lol\": \"laugh out loud\", \n",
    "    \"idk\": \"I don't know\", \n",
    "    \"brb\": \"be right back\", \n",
    "    \"btw\": \"by the way\", \n",
    "    \"imo\": \"in my opinion\", \n",
    "    \"smh\": \"shaking my head\", \n",
    "    \"fyi\": \"for your information\", \n",
    "    \"np\": \"no problem\", \n",
    "    \"ikr\": \"I know right\", \n",
    "    \"asap\": \"as soon as possible\", \n",
    "    \"bff\": \"best friend forever\", \n",
    "    \"gg\": \"good game\", \n",
    "    \"hmu\": \"hit me up\", \n",
    "    \"rofl\": \"rolling on the floor laughing\" \n",
    "} \n",
    "\n",
    "# Contractions dictionary \n",
    "contractions_dict = { \n",
    "    \"wasn't\": \"was not\", \n",
    "    \"isn't\": \"is not\", \n",
    "    \"aren't\": \"are not\", \n",
    "    \"weren't\": \"were not\", \n",
    "    \"doesn't\": \"does not\", \n",
    "    \"don't\": \"do not\", \n",
    "    \"didn't\": \"did not\", \n",
    "    \"can't\": \"cannot\", \n",
    "    \"couldn't\": \"could not\", \n",
    "    \"shouldn't\": \"should not\", \n",
    "    \"wouldn't\": \"would not\", \n",
    "    \"won't\": \"will not\", \n",
    "    \"haven't\": \"have not\", \n",
    "    \"hasn't\": \"has not\", \n",
    "    \"hadn't\": \"had not\", \n",
    "    \"i'm\": \"i am\", \n",
    "    \"you're\": \"you are\", \n",
    "    \"he's\": \"he is\", \n",
    "    \"she's\": \"she is\", \n",
    "    \"it's\": \"it is\", \n",
    "    \"we're\": \"we are\", \n",
    "    \"they're\": \"they are\", \n",
    "    \"i've\": \"i have\", \n",
    "    \"you've\": \"you have\", \n",
    "    \"we've\": \"we have\", \n",
    "    \"they've\": \"they have\", \n",
    "    \"i'd\": \"i would\", \n",
    "    \"you'd\": \"you would\", \n",
    "    \"he'd\": \"he would\", \n",
    "    \"she'd\": \"she would\", \n",
    "    \"we'd\": \"we would\", \n",
    "    \"they'd\": \"they would\", \n",
    "    \"i'll\": \"i will\", \n",
    "    \"you'll\": \"you will\", \n",
    "    \"he'll\": \"he will\", \n",
    "    \"she'll\": \"she will\", \n",
    "    \"we'll\": \"we will\", \n",
    "    \"they'll\": \"they will\", \n",
    "    \"let's\": \"let us\", \n",
    "    \"that's\": \"that is\", \n",
    "    \"who's\": \"who is\", \n",
    "    \"what's\": \"what is\", \n",
    "    \"where's\": \"where is\", \n",
    "    \"when's\": \"when is\", \n",
    "    \"why's\": \"why is\" \n",
    "} \n",
    "\n",
    "# Remove URLs \n",
    "def remove_urls(text): \n",
    "    return re.sub(r'http\\S+|www\\S+', '', text) \n",
    "\n",
    "# Remove HTML tags \n",
    "def remove_html(text): \n",
    "    return BeautifulSoup(text, \"html.parser\").get_text() \n",
    "\n",
    "# Remove emojis \n",
    "def remove_emojis(text): \n",
    "    return emoji.replace_emoji(text, replace='') \n",
    "\n",
    "# Replace slang words \n",
    "def replace_slang(text): \n",
    "    for slang, replacement in slang_dict.items(): \n",
    "        text = re.sub(r'\\b' + re.escape(slang) + r'\\b', replacement, text, flags=re.IGNORECASE) \n",
    "    return text \n",
    "\n",
    "# Expand contractions \n",
    "def replace_contractions(text): \n",
    "    for contraction, expansion in contractions_dict.items(): \n",
    "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expansion, text, flags=re.IGNORECASE) \n",
    "    return text \n",
    "\n",
    "# Remove punctuation \n",
    "def remove_punctuation(text): \n",
    "    return text.translate(str.maketrans('', '', string.punctuation)) \n",
    "\n",
    "# Remove numbers \n",
    "def remove_numbers(text): \n",
    "    return re.sub(r'\\d+', '', text) \n",
    "\n",
    "# Remove stopwords \n",
    "def remove_stopwords(text): \n",
    "    words = text.split() \n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words] \n",
    "    return \" \".join(filtered_words) \n",
    "\n",
    "# POS tagging for lemmatization \n",
    "def get_wordnet_pos(nltk_tag): \n",
    "    if nltk_tag.startswith('J'): \n",
    "        return wordnet.ADJ \n",
    "    elif nltk_tag.startswith('V'): \n",
    "        return wordnet.VERB \n",
    "    elif nltk_tag.startswith('N'): \n",
    "        return wordnet.NOUN \n",
    "    elif nltk_tag.startswith('R'): \n",
    "        return wordnet.ADV \n",
    "    else: \n",
    "        return wordnet.NOUN \n",
    "\n",
    "# Lemmatization \n",
    "def lemmatize_text(text): \n",
    "    words = word_tokenize(text) \n",
    "    pos_tags = pos_tag(words) \n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags] \n",
    "    return \" \".join(lemmatized_words) \n",
    "\n",
    "# Preprocessing pipeline \n",
    "def preprocess_text(text): \n",
    "    text = text.lower() \n",
    "    text = remove_urls(text) \n",
    "    text = remove_html(text) \n",
    "    text = remove_emojis(text) \n",
    "    text = replace_slang(text) \n",
    "    text = replace_contractions(text) \n",
    "    text = remove_punctuation(text) \n",
    "    text = remove_numbers(text) \n",
    "    text = remove_stopwords(text) \n",
    "    text = lemmatize_text(text) \n",
    "    return text \n",
    "\n",
    "# Load dataset \n",
    "df = pd.read_csv(\"UNITENReview.csv\") \n",
    "df = df[[\"Review\"]]  \n",
    "\n",
    "df[\"processed\"] = df[\"Review\"].apply(preprocess_text) \n",
    "df.to_csv(\"Processed_UNITENReview.csv\", index=False) \n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf918b-a277-4100-8839-1265702930a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
